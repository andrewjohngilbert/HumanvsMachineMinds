<!DOCTYPE html>
<html>
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-81D6829LG0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-81D6829LG0');
</script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="This paper investigates how humans and AI models differ in their ability to recognise activities from video segments in challenging ego-centric scenarios. Ego-centric videos provide enhanced information about objects within an actor’s immediate affordance  and their interactions via hands , yet they often offer less contextual information. This reduced context may influence both the visual features and cognitive strategies  employed during action recognition.">
  <meta property="og:title" content="DEAR: Depth-Estimated Action Recognition"/>
  <meta property="og:description" content="This paper investigates how humans and AI models differ in their ability to recognise activities from video segments in challenging ego-centric scenarios. Ego-centric videos provide enhanced information about objects within an actor’s immediate affordance  and their interactions via hands , yet they often offer less contextual information. This reduced context may influence both the visual features and cognitive strategies  employed during action recognition."/>
  <meta property="og:url" content="https://andrewjohngilbert.github.io/HumanvsMachineMinds/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="assets/HumanvsMachineMindsTeaserWide.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="DEAR: Depth-Estimated Action Recognition">
  <meta name="twitter:description" content="This paper investigates how humans and AI models differ in their ability to recognise activities from video segments in challenging ego-centric scenarios. Ego-centric videos provide enhanced information about objects within an actor’s immediate affordance  and their interactions via hands , yet they often offer less contextual information. This reduced context may influence both the visual features and cognitive strategies  employed during action recognition.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="asserts/HumanvsMachineMindsTeaserWide.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Machine Learning, Human Understanding, Video Understanding,andrew Gilbert, Sadegh Rahmani, Quoc Vuong, Frank Guerin, Filip Rybansky, Human vs Machine Minds, ECCV 2024, HCV 2024, Action Recognition, Ego-centric Videos, Depth Estimation">
  <meta name="author" content="Sadegh Rahmani, Filip Rybansky, Quoc Vuong, Frank Guerin, Andrew Gilbert">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DEAR: Depth-Estimated Action Recognition</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Human vs. Machine Minds: Ego-Centric Action Recognition Compared</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/sadegh-rahmani" target="_blank">Sadegh Rahmani*</a>[1]<sup></sup>,</span>
                <span class="author-block">
                  <a href="F" target="_blank">Filip Rybansky*</a>[2]<sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://www.ncl.ac.uk/medical-sciences/people/profile/quocvuong.html" target="_blank">Quoc Vuong</a>[2]<sup></sup>,</span>
                <span class="author-block">
                  <a href="https://www.surrey.ac.uk/people/frank-guerin" target="_blank">Frank Guerin</a>[1]<sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>[1]<sup></sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">* These authors contributed equally <br> [1] University of Surrey [2] University of Newcastle<br> <a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank">IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025</a> <a href="https://marworkshop.github.io/cvpr25/index.html" target="_blank">Workshop on Multimodal Algorithmic Reasoning (MAR'25)</a>              
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://andrewjohngilbert.github.io/HumanvsMachineMinds/assets/HumanvsMachineMindsPaper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Poster PDF link -->
                     <!--
                   <span class="link-block">
                      <a href="assets/DEAR HCV Poster.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>
                -->
 
                  <!-- Github link -->
                <span class="link-block">
                    <a href="https://github.com/SadeghRahmaniB/Epic-ReduAct" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code & Dataset</span>
                  </a>
                </span>
              

                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/HumanvsMachineMindsSystem.jpg">
      <h2 class="subtitle has-text-centered">
        Our research pipeline, outlines our approach to comparing human and AI performance in ego-centric video action recognition. We began by employing a classifier to pre-select easy and hard video sets. To enable a comparison between how human and AI models recognise activities in video, we artificially and systematically reduced the video's spatial resolution. Then, using human participants and an AI model as classifiers, we evaluate and compare human and AI performance on these spatially reduced videos to quantify the difference in recognition between the human and AI models. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser Image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Humans reliably surpass the performance of the most advanced AI models in action recognition, especially in real-world scenarios with low resolution, occlusions, and visual clutter. These models are somewhat similar to humans in using architecture that allows hierarchical feature extraction. However, they prioritise different features, leading to notable differences in their recognition. This study investigated these differences by introducing Epic ReduAct, a dataset derived from Epic-Kitchens-100. It consists of Easy and Hard ego-centric videos across various action classes. Critically, our dataset incorporates the concepts of Minimal Recognisable Configuration (MIRC) and sub-MIRC derived by progressively reducing the spatial content of the action videos across multiple stages. This enables a controlled evaluation of recognition difficulty for humans and AI models. This study examines the fundamental differences between human and AI recognition processes. While humans, unlike AI models, demonstrate proficiency in recognising hard videos, they experience a sharp decline in recognition ability as visual information is reduced, ultimately reaching a threshold beyond which recognition is no longer possible. In contrast, the AI models examined in this study appeared to exhibit greater resilience within this specific context, with recognition confidence decreasing gradually or, in some cases, even increasing at later reduction stages. These findings suggest that the limitations observed in human recognition do not directly translate to AI models, highlighting the distinct nature of their processing mechanisms.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Dataset-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Epic ReduAct Dataset</h2>
      <a href="https://github.com/SadeghRahmaniB/Epic-ReduAct" target="_blank">Epic ReduAct Dataset download</a>
      <img src="assets/HumanvsMachineMindsEpicReduActDetails.jpg">
      <h2 class="subtitle has-text-centered">
        To enable out investigation, we first created an Easy and Hard subsets of the Epic-Kitchens dataset that represents different levels of activity recognition difficulty for AI models. Each set comprises of 18 Epic-Kitchens videos with a mean duration of 2.35s (Standard Deviation (SD) duration = 1.11s), to enable comparisons between human and AI model performance on distinct difficulty levels. Next, we conducted online experiments to systematically reduce the spatial information of the 18 Easy and 18 Hard videos (36 total) across eight hierarchical levels to identify MIRCs. The process is shown in below for a video with the GT label close. At Level 0, we spatially cropped a region of the video that best encompassed the entirety of each video. At Level 1, frames from each parent video were cropped at the four corners, generating four child sub-videos per original. Levels 2 through 7 involved recursively applying this corner-cropping method to each subsequent generation of parent videos.  
      </h2>
      <img src="assets/HumanvsMachineMindsTeaser.jpg">
    </div>
  </div>
</section>
<!-- End Dataset-->

<!-- Recongtion Gap-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Recongtion Gap</h2>
      <img src="assets/HumanvsMachineMindsRecogGap.jpg">
      <h2 class="subtitle has-text-centered">
        This image presents the recognition-gap frequency distribution for the Easy, Hard and combined sets (a,b,c), which allows for comparison between humans and AI model. Our results show a similar distribution pattern to previous work with images (d). 
        Similarly, AI models exhibit some improvement in image recognition, whereas human accuracy consistently declines. Humans also experience a sharper decrease in recognition performance compared to AI models (d). Our results further show that humans are susceptible to substantial losses in recognition confidence. In contrast, spatial reductions can enhance the AI model’s ability to detect actions, as evidenced by negative recognition gaps. The frequency distributions are also broader for humans compared to the AI model, showing more diverse reductions than the AI model reductions in recognition gaps, which are more gradual. These findings indicate that, despite advancements in AI models, the gap between human and machine recognition capabilities persists. 
      </h2>
    </div>
  </div>
</section>
<!-- End Recongtion Gap-->

<!-- Paper poster -->
<!--
  <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="assets/DEAR HCV Poster.pdf" width="100%" height="1000">
          </iframe>
        
      </div>
    </div>
  </section>
-->
  <!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{Rahmani:HumanvsMachine:CVPRWS:2025,
        AUTHOR = Rahmani, Sadegh and Rybansky, Filip and Vuong, Quoc and Guerin, Frank and Gilbert, Andrew ",
        TITLE = "Human vs. Machine Minds: Ego-Centric Action Recognition Compared",
        BOOKTITLE = "IEEE/CVF Conference on Computer Vision and Pattern Recognition - Workshop on Multimodal Algorithmic Reasoning (MAR'25)",
        YEAR = "2025",
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
